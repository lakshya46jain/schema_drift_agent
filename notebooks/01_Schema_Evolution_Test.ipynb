{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb685850-1314-43c7-bd59-669f779edb53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [01] Schema Evolution Test\n",
    "\n",
    "This notebook demonstrates how Delta Lake enforces and evolves schemas using `.option(\"mergeSchema\")` and `.option(\"overwriteSchema\")`. All code is assuming a valid mount (e.g., `/mnt/...`) is configured as the write path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35c945e-279a-4641-9363-e084deaff9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the write path. Defining the Delta table storage path\n",
    "path = \"/Volumes/workspace/default/schema_evolution_test\"\n",
    "print(\"Delta write path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf1bd4b-77b0-49d3-be85-0748bc39f405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create base DataFrame with two columns\n",
    "df_base = spark.createDataFrame(\n",
    "    [(1, \"Alice\")],\n",
    "    [\"id\", \"name\"]\n",
    ")\n",
    "\n",
    "# Overwrite any existing table at the path\n",
    "df_base.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(path)\n",
    "\n",
    "# Confirm the schema\n",
    "print(\"Initial schema:\")\n",
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd9e2e9-1b95-4008-be9a-44d670f3dd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Introduce a new 'age' column\n",
    "df_new = spark.createDataFrame([(2, \"Bob\", 30)], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "try:\n",
    "    df_new.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(path)\n",
    "except Exception as e:\n",
    "    print(\"Expected enforcement error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617f538b-c34e-49b0-b2de-c5cda09f19d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(path)\n",
    "\n",
    "print(\"Schema after mergeSchema:\")\n",
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8f2dbf-138e-4d22-8189-b84415e37057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Without mergeSchema, writes fail if schema mismatch exists (extra column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc2c81e-5c09-4f0f-922a-ef3447d9d22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change 'age' type to string\n",
    "df_type_change = spark.createDataFrame([(3, \"Charlie\", \"35\")], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "df_type_change.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(path)\n",
    "\n",
    "print(\"Schema after type change attempt:\")\n",
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53457a9c-e908-4cf0-8141-788eaf57ba0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta supports some safe type promotions—review the schema to verify behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfc3ed4-f661-4487-ba0e-eb1209acc0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename 'name' to 'full_name'\n",
    "df_renamed = spark.read.format(\"delta\").load(path) \\\n",
    "    .withColumnRenamed(\"name\", \"full_name\")\n",
    "\n",
    "df_renamed.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(path)\n",
    "\n",
    "print(\"Schema after overwriteSchema rename:\")\n",
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7d6a83-6eaa-4c24-a170-dccb060d9bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Renames (and other structural changes) require overwriteSchema=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6660a1-af64-43cf-95ec-78e5c4dd1c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Scenario | Feature Tested | Expected Behavior | Notes |\n",
    "|----------|----------------|--------------------|-------|\n",
    "| Append data with same schema | Baseline | ✅ Data appends successfully | Confirms Delta can accept schema-matching data without error |\n",
    "| Append data with **new column** | Schema Enforcement | ❌ Fails unless `.option(\"mergeSchema\", \"true\")` is set | Delta blocks writes with unknown columns by default |\n",
    "| Append data with **new column** + mergeSchema | Schema Evolution | ✅ Delta adds the new column to the table schema | Only supported for additions; no error thrown |\n",
    "| Append data with **changed type** | Schema Evolution (nullable/upcast) | ✅ Sometimes allowed, depending on compatibility (e.g., INT → LONG) | Delta allows upcasts; stricter type changes require overwrite or DDL |\n",
    "| Rename column | Not supported by mergeSchema | ❌ Delta throws error unless `.option(\"overwriteSchema\", \"true\")` is used | Renaming needs full overwrite to avoid data integrity issues |\n",
    "| Overwrite table with new schema | Overwrite Schema           | ✅ Succeeds with `.option(\"overwriteSchema\", \"true\")` | Replaces entire schema, used for renames or redefinitions |\n",
    "| Column removal | Not supported by mergeSchema | ❌ Must use overwriteSchema or DDL | Delta does not auto-remove columns; must be done manually |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ac7b1a-bcb0-4d48-ba08-107c08589108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Takeaway\n",
    "- Delta Lake supports automatic schema evolution **only for column additions** and some **safe type promotions** when ```.option(\"mergeSchema\", \"true\")``` is used.  \n",
    "\n",
    "- Schema enforcement is strict by default to prevent data corruption.  \n",
    "\n",
    "- Advanced changes like renaming or deleting columns require ```.overwriteSchema``` or manual DDL updates."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Schema_Evolution_Test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
