{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3add6dc5-67fb-4251-a09c-0ba266dab787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [00] Project Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4ec440-79fe-4c6e-af97-e2445d055f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Objective\n",
    "\n",
    "Build an LLM-powered **Schema Drift Detection & Auto‑Resolution Agent** in Databricks. The goal is to **automatically identify** structural changes in Delta table schemas and **suggest code updates** (PySpark or SQL) to handle them. This helps avoid broken pipelines and reduces manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f96e8d6-397c-4cb7-a8ef-81bfb2042d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "- **Schema drift** occurs when data schemas change unexpectedly (e.g., added, removed, renamed columns, or changed data types).\n",
    "- These changes can **break ETL pipelines**, cause downstream failures, and require manual fixes.\n",
    "- We need a system that can **detect drift**, **explain it to humans**, and **generate resolution code** automatically within Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c219889-0947-4580-a5a9-cdb0e91799dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### High Level Architecture\n",
    "\n",
    "1. **Fetch** current and new table schemas from Delta.\n",
    "2. **Compare** schemas to compute a structured diff.\n",
    "3. **Generate prompt** for LLM, describing the schema changes.\n",
    "4. **LLM returns** a code snippet for resolution (e.g., modify DataFrame or ALTER TABLE).\n",
    "5. **Display** the suggestion in notebook; allow engineer to **review and execute**.\n",
    "6. *(Future scope)* Automatically apply changes or log outputs for monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "347b59bf-ab22-4bb2-b2f1-e60de1a16a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Milestones Overview\n",
    "\n",
    "| Week | Main Focus |\n",
    "|------|------------|\n",
    "| 1    | Research, planning, schema evolution tests |\n",
    "| 2    | Schema comparison logic (PySpark) |\n",
    "| 3    | Prompt engineering + LLM integration |\n",
    "| 4    | End-to-end prototype development |\n",
    "| 5    | Testing, validation, prompt refinement |\n",
    "| 6    | Documentation, demo, final wrap-up |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d386c462-a034-4fcc-80e3-d00f50db934c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning: Delta Schema Enforcement & Evolution\n",
    "**Link**: https://www.databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\n",
    "\n",
    "#### 1. Schema Enforcement Definition and Behavior\n",
    "\n",
    "Delta Lake’s schema enforcement (aka schema validation) ensures data quality by rejecting any writes that don’t match the expected schema of a target table. It acts like a gatekeeper, blocking writes with extra columns or mismatched types\n",
    "\n",
    "#### 2. Enforcement Rules\n",
    "\n",
    "The blog outlines these key rules for schema enforcement:\n",
    "- **No additional columns** in the incoming DataFrame that aren't already present in the table.\n",
    "\n",
    "- **Missing columns** are allowed and filled with ```NULL```.\n",
    "\n",
    "- **Data types must match exactly** - strict type matching is enforced.\n",
    "\n",
    "- **Column names must match case-insensitively**, and Delta Lake forbids having both ```Foo``` and ```foo``` as seperate columns.\n",
    "\n",
    "#### 3. Schema Evolution Mechanisms\n",
    "When ```.option(\"mergeSchema\", \"true\")``` is used:\n",
    "  - New columns added in the DataFrame are automatically appended to the table’s schema.\n",
    "\n",
    "  - Nested struct fields are also supported.\n",
    "\n",
    "  - Type changes from nullable to another type (e.g. ```NullType``` → ```StringType```) and certain upcasts (```Byte → Short → Integer```) are handled.\n",
    "  This makes the evolution seamless for common changes.\n",
    "\n",
    "#### 4. Use Cases & Trade-offs\n",
    "- **Schema enforcement** is ideal for production-quality tables feeding downstream systems like ML models and BI dashboards—offering strong data integrity and preventing accidental schema drift.\n",
    "\n",
    "- **Schema evolution** is useful when you intend to change schemas, letting you add columns without manual intervention. However:\n",
    "  - It doesn’t handle **column removal**, **in-place type changes**, or **renames** (especially case changes)—these require ```.option(\"overwriteSchema\", \"true\")``` or DDL commands.\n",
    "  - It’s purposely limited so as not to silently break downstream expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "541c49c2-f2eb-42ed-b0f8-7aa2aeb6a517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning: Auto Loader Schema Drift\n",
    "**Link:** https://community.databricks.com/t5/technical-blog/schema-management-and-drift-scenarios-via-databricks-auto-loader/ba-p/63393\n",
    "\n",
    "#### 1. Schema Inference Mechanism\n",
    "- Auto Loader **samples up to 50 GB or 1,000 files** to infer the schema for the input directory.  \n",
    "\n",
    "- It writes inferred schemas into a ```_schemas``` folder under the configured ```cloudFiles.schemaLocation```—this becomes your source of truth for schema evolution over time\n",
    "\n",
    "#### 2. Supported File Formats & Type Inference\n",
    "- **JSON, CSV, XML:** Auto Loader infers everything as strings unless you enable ```.option(\"cloudFiles.inferColumnTypes\", \"true\")```.  \n",
    "\n",
    "- **Text, Binary:** These formats don't support evolution.\n",
    "\n",
    "- **Parquet, Avro:** Typed formats retain their native data types and are merged during sampling. On type conflicts, it chooses the widest type (e.g., long over int), unless overridden by ```schemaHints```.\n",
    "\n",
    "#### 3. Automatic Schema Evolution  \n",
    "- When Auto Loader detects **new columns**, it:\n",
    "  1. Stops the stream with an `UnknownFieldException`.\n",
    "  2. Merges the new column(s) into the schema, placing them at the end.\n",
    "  3. Keeps existing column types intact.  \n",
    "- **Note:** Instructs a pipeline restart (e.g., via Lakeflow Jobs) to resume with updated schema.\n",
    "\n",
    "#### 4. ```cloudFiles.schemaEvolutionMode``` Control  \n",
    "Auto Loader supports several modes:\n",
    "\n",
    "| Mode | Behavior |\n",
    "|------|----------|\n",
    "| `addNewColumns` | (Default) Stream **fails**, schema file updated to include new columns. |\n",
    "| `rescue` | Stream continues; unexpected fields go into a `_rescued_data` column. |\n",
    "| `failOnNewColumns` | Stream fails; schema is NOT updated until manually changed. |\n",
    "| `none` | Stream continues; new columns are ignored unless `rescuedDataColumn` is set. |\n",
    "\n",
    "- The default behavior is `addNewColumns` unless a user-supplied schema is provided, in which case the default changes to `none`.\n",
    "\n",
    "#### 5. `_rescued_data` Column  \n",
    "- If using ```rescue``` mode (or enabling ```rescuedDataColumn```), unmatched fields are captured in a ```_rescued_data``` column rather than being dropped.  \n",
    "- You can rename this column via the ```rescuedDataColumn``` option.\n",
    "\n",
    "#### 6. Partition Columns Are Ignored in Drift  \n",
    "- Auto Loader can detect Hive-style partition columns (e.g., ```/date=2025-01-01/```).  \n",
    "- **Partition evolution is not supported**: new partitions will not be added to the schema unless manually specified using ```cloudFiles.partitionColumns```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0349f884-0eb0-4326-a9da-0601a664b0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How Research Informs the Project\n",
    "\n",
    "#### 1. Schema Enforcement & Evolution (Delta Lake)\n",
    "- Clarifies which schema changes Delta *handles automatically*—such as column additions, nullable upcasts, and nested fields—ensuring the agent avoids duplicated work.\n",
    "\n",
    "- Defines the boundaries of *unsupported changes* (e.g., column drops, renames, type conversions) that require explicit intervention, allowing the agent to detect and prompt for these specific adjustments.\n",
    "\n",
    "- Supports development of logic that differentiates between \"safe\" drift (handled by system) and \"risky\" drift (requiring agent-generated code), improving system reliability\n",
    "\n",
    "#### 2. Auto Loader Schema Management (Streaming Context)\n",
    "- Highlights how Auto Loader samples schemas and manages drift through ```_schemas``` metadata and ```cloudFiles.schemaEvolutionMode``` configurations (e.g., ```addNewColumns```, ```rescue```), which enables the agent to align its behavior with real-time ingestion pipelines.\n",
    "\n",
    "- Identifies scenarios—like data captured in ```_rescued_data```, unsupported partition-schema changes, or streaming failures—where the agent should suggest specific modes or extraction logic, enhancing streaming robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd288468-0836-408f-bfda-99bebdb5436a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema Drift Types to Detect\n",
    "\n",
    "We categorize schema drift types into the following categories:\n",
    "\n",
    "1. **Column Added**\n",
    "    - A new column appears in the updated schema.\n",
    "    - Example: ```age``` (IntegerType) added.\n",
    "\n",
    "2. **Column Removed**\n",
    "    - A column present in the old schema is missing in the new one.\n",
    "    - Example: ```name``` (StringType) removed.\n",
    "\n",
    "3. **Column Type Changed**\n",
    "    - A column exists in both schemas, but its data type is different.\n",
    "    - Example: ```id```: IntegerType → LongType\n",
    "\n",
    "Note: Complex changes like **nested fields**, **renamed columns**, or **reordered fields** are not handled in Week 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26cd2d0d-8fc7-4f9d-8350-83b6345a0477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Design Plan for Schema Drift Agent\n",
    "\n",
    "The Schema Drift Agent will consist of:\n",
    "\n",
    "1. **Schema Comparison Engine**\n",
    "   - Written in Python (`schema_diff.py`)\n",
    "   - Takes two JSON schemas: old and new\n",
    "   - Uses `load_schema()` to read them, and `compute_schema_diff()` to compare them\n",
    "   - Produces a list of changes like:\n",
    "     - Column X added\n",
    "     - Column Y removed\n",
    "     - Column Z changed type\n",
    "     - Column X was renamed to A\n",
    "\n",
    "2. **Prompt Generation Logic**\n",
    "   - Based on schema diffs, we will construct prompts for an LLM\n",
    "   - Example: “Column `age` added. Please provide PySpark code to handle this.”\n",
    "\n",
    "3. **Test Schema Setup**\n",
    "   - We simulate schema drift using files like:\n",
    "     - `schema_test_old.json`\n",
    "     - `schema_test_new.json`\n",
    "   - These files contain simplified versions of table schemas for easy validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6177c308-2938-4028-b66d-bea02d83c712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rename Schema Drift Detection\n",
    "\n",
    "#### Research & Insights\n",
    "\n",
    "- **Estuary (https://estuary.dev/blog/schema-drift/)** underscores that real-world data pipelines must detect structural changes—like renames—early, because upstream variance can break downstream systems.\n",
    "- **Metaplane (https://www.metaplane.dev/blog/three-ways-to-track-schema-drift-in-snowflake)** recommends frequent schema snapshot comparison to catch drift types including renames.\n",
    "- **Snowflake/Matia Guide (https://www.matia.io/blog/how-to-build-a-schema-drift-alert-system)** highlights renames as a critical class of schema drift, and suggests treating them as a combination of removal and addition unless intelligently paired.\n",
    "\n",
    "**Key Takeaway:**  \n",
    "To detect renames, we must map removed → added column pairs, ideally using heuristics (name similarity + type matching), with optional manual hints.\n",
    "\n",
    "#### Design Approach\n",
    "\n",
    "**Objective**: Extend ```compute_schema_diff()``` to mark and report renamed columns.\n",
    "\n",
    "1. **Extend ```diff``` result structure**:\n",
    "   - Add a ```renamed``` list alongside ```added```, ```removed```, ```type_changed```.\n",
    "\n",
    "2. **Develop ```detect_renames(diff, old_fields, new_fields, hints=None)```**:\n",
    "   - **Manual override**: use ```rename_hints.json``` map (e.g., ```\"fullname\": \"name\"```).\n",
    "   - **Automatic heuristic**: match removed and added names by similarity (e.g., ```startswith```, levenshtein) **and** require same type.\n",
    "   - If renamed → add ```{\"from\": old, \"to\": new}```, and remove from ```added```/```removed```.\n",
    "\n",
    "3. **Integrate detection**:\n",
    "   - Run ```detect_renames()``` after computing the flat/nested diff, before returning result.\n",
    "\n",
    "#### Update (17 July 2025): Heuristic Rename Detection Removed\n",
    "\n",
    "I have removed **heuristic-based rename detection** from the schema drift agent due to inconsistent behavior and unreliable matches in edge cases. While the approach attempted to pair removed and added fields using name similarity (e.g., ```startswith``` logic), it led to:\n",
    "- False positives for unrelated fields with similar names\n",
    "- Missed matches when nested paths or casing differences were involved\n",
    "- Complexity in test coverage and debugging\n",
    "\n",
    "**Current Strategy:**  \n",
    "I now rely **only on manual rename hints** (```rename_hints``` dictionary) to detect renamed columns. These mappings explicitly specify expected renames (e.g., ```\"created_at\" → \"account_created\"```) and allow for clear, controlled detection. If the renamed field’s type also changes, that is captured in the ```type_changed``` section.\n",
    "\n",
    "This change simplifies logic, improves precision, and ensures the agent aligns with real-world expectations in production-grade data platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b57b43b5-69c5-4493-8075-0530c8242e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why Reordered Column Drift Is Not Detected\n",
    "\n",
    "Explored whether to include **field reordering** (e.g., changing column order from ```[\"id\", \"name\"]``` to ```[\"name\", \"id\"]```) as a schema drift type.\n",
    "\n",
    "#### Decision:\n",
    "Reordering is **not treated as drift**, and will not be detected or resolved by the agent.\n",
    "\n",
    "#### Reasoning:\n",
    "- **Delta Lake & Spark**: Column order is not enforced; schema validation only checks for name and type matches.\n",
    "- **Parquet/Avro formats**: Internally store columnar data by name, not order.\n",
    "- **SQL Engines & BI Tools**: Reference columns by name — order does not affect execution.\n",
    "- **Reordering does not break pipelines**, transformations, or queries in modern systems.\n",
    "\n",
    "#### Exceptions:\n",
    "- CSV-based workflows (order-sensitive by design)\n",
    "- Scripts using positional access like ```df.iloc[:, 0]``` (bad practice)\n",
    "- Legacy fixed-position APIs (rare)\n",
    "\n",
    "Because these cases are uncommon in modern data platforms like Databricks, we classify reordering as **non-breaking drift**.\n",
    "\n",
    "It may be flagged visually by users, but **no automated resolution or LLM prompt is required**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "763ff86e-377c-4b21-8e73-cd3881073d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rename + Type Change Handling (Design Update)\n",
    "\n",
    "#### Problem\n",
    "\n",
    "My initial `detect_renames()` implementation only recognized renamed columns if their data types **remained unchanged**. As a result, important drifts like:\n",
    "\n",
    "- `created_at` → `account_created` (`TimestampType` → `DateType`)\n",
    "- `last_login` → `lastLogin` (`TimestampType` → `StringType`)\n",
    "\n",
    "were not marked as renames, and their type changes were also missed.\n",
    "\n",
    "This led to incorrect or incomplete drift reports and caused several test assertions to fail.\n",
    "\n",
    "#### Root Cause\n",
    "\n",
    "The detection logic skipped rename matches when: \n",
    "```if old_fields_map.get(old_name) == new_fields_map.get(new_name):```\n",
    "\n",
    "This condition only allowed renames where the types matched exactly, ignoring valid cases where both the name changed and type evolved.\n",
    "\n",
    "#### Solution\n",
    "I updated the ```detect_renames()``` function to:\n",
    "\n",
    "Allow renames even if types differ\n",
    "\n",
    "Add an entry to ```type_changed``` when a renamed field’s type also changes\n",
    "\n",
    "Apply this logic to both manual rename hints and heuristic rename matches\n",
    "\n",
    "I also added a ```flatten_fields()``` utility to recursively support nested structs, so renames and type changes are detected for fields like ```metadata.score``` and ```preferences.notifications```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "795e78d5-4ddc-454b-b688-6291fddd5124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Future Scope: Struct-Level Rename Detection\n",
    "\n",
    "Currently, the schema drift agent detects renamed fields using **fully qualified field names** (e.g., ```metadata.source``` → ```metadata.origin```) and relies on **explicit rename hints** for each renamed field.\n",
    "\n",
    "However, it does **not support renames at the struct level** (e.g., ```metadata``` → ```meta_info```) unless all nested fields are individually mapped in ```rename_hints```. This can lead to false positives like:\n",
    "\n",
    "- ```\"metadata.source\"``` showing up as **removed**\n",
    "- ```\"meta_info.source\"``` showing up as **added**\n",
    "\n",
    "even though the structure and types may be identical.\n",
    "\n",
    "**Planned enhancement:**  \n",
    "Support intelligent struct-level rename detection by:\n",
    "- Matching renamed parent structs based on field overlap and type similarity\n",
    "- Automatically resolving child field renames when parent structs are identified as renamed\n",
    "- Reducing the need for manually listing every nested rename in ```rename_hints```\n",
    "\n",
    "This feature would improve drift detection for large or deeply nested schemas and make the agent more scalable and intelligent."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Project_Planning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
