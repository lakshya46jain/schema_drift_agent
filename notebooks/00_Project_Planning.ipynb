{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3add6dc5-67fb-4251-a09c-0ba266dab787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [00] Project Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4ec440-79fe-4c6e-af97-e2445d055f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Objective\n",
    "\n",
    "Build an LLM-powered **Schema Drift Detection & Auto‑Resolution Agent** in Databricks. The goal is to **automatically identify** structural changes in Delta table schemas and **suggest code updates** (PySpark or SQL) to handle them. This helps avoid broken pipelines and reduces manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f96e8d6-397c-4cb7-a8ef-81bfb2042d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Problem Statement\n",
    "\n",
    "- **Schema drift** occurs when data schemas change unexpectedly (e.g., added, removed, renamed columns, or changed data types).\n",
    "- These changes can **break ETL pipelines**, cause downstream failures, and require manual fixes.\n",
    "- We need a system that can **detect drift**, **explain it to humans**, and **generate resolution code** automatically within Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c219889-0947-4580-a5a9-cdb0e91799dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### High Level Architecture\n",
    "\n",
    "1. **Fetch** current and new table schemas from Delta.\n",
    "2. **Compare** schemas to compute a structured diff.\n",
    "3. **Generate prompt** for LLM, describing the schema changes.\n",
    "4. **LLM returns** a code snippet for resolution (e.g., modify DataFrame or ALTER TABLE).\n",
    "5. **Display** the suggestion in notebook; allow engineer to **review and execute**.\n",
    "6. *(Future scope)* Automatically apply changes or log outputs for monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "347b59bf-ab22-4bb2-b2f1-e60de1a16a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Milestones Overview\n",
    "\n",
    "| Week | Main Focus |\n",
    "|------|------------|\n",
    "| 1    | Research, planning, schema evolution tests |\n",
    "| 2    | Schema comparison logic (PySpark) |\n",
    "| 3    | Prompt engineering + LLM integration |\n",
    "| 4    | End-to-end prototype development |\n",
    "| 5    | Testing, validation, prompt refinement |\n",
    "| 6    | Documentation, demo, final wrap-up |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d386c462-a034-4fcc-80e3-d00f50db934c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning: Delta Schema Enforcement & Evolution\n",
    "**Link**: https://www.databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\n",
    "\n",
    "#### 1. Schema Enforcement Definition and Behavior\n",
    "\n",
    "Delta Lake’s schema enforcement (aka schema validation) ensures data quality by rejecting any writes that don’t match the expected schema of a target table. It acts like a gatekeeper, blocking writes with extra columns or mismatched types\n",
    "\n",
    "#### 2. Enforcement Rules\n",
    "\n",
    "The blog outlines these key rules for schema enforcement:\n",
    "- **No additional columns** in the incoming DataFrame that aren't already present in the table.\n",
    "\n",
    "- **Missing columns** are allowed and filled with ```NULL```.\n",
    "\n",
    "- **Data types must match exactly** - strict type matching is enforced.\n",
    "\n",
    "- **Column names must match case-insensitively**, and Delta Lake forbids having both ```Foo``` and ```foo``` as seperate columns.\n",
    "\n",
    "#### 3. Schema Evolution Mechanisms\n",
    "When ```.option(\"mergeSchema\", \"true\")``` is used:\n",
    "  - New columns added in the DataFrame are automatically appended to the table’s schema.\n",
    "\n",
    "  - Nested struct fields are also supported.\n",
    "\n",
    "  - Type changes from nullable to another type (e.g. ```NullType``` → ```StringType```) and certain upcasts (```Byte → Short → Integer```) are handled.\n",
    "  This makes the evolution seamless for common changes.\n",
    "\n",
    "#### 4. Use Cases & Trade-offs\n",
    "- **Schema enforcement** is ideal for production-quality tables feeding downstream systems like ML models and BI dashboards—offering strong data integrity and preventing accidental schema drift.\n",
    "\n",
    "- **Schema evolution** is useful when you intend to change schemas, letting you add columns without manual intervention. However:\n",
    "  - It doesn’t handle **column removal**, **in-place type changes**, or **renames** (especially case changes)—these require ```.option(\"overwriteSchema\", \"true\")``` or DDL commands.\n",
    "  - It’s purposely limited so as not to silently break downstream expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "541c49c2-f2eb-42ed-b0f8-7aa2aeb6a517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning: Auto Loader Schema Drift\n",
    "**Link:** https://community.databricks.com/t5/technical-blog/schema-management-and-drift-scenarios-via-databricks-auto-loader/ba-p/63393\n",
    "\n",
    "#### 1. Schema Inference Mechanism\n",
    "- Auto Loader **samples up to 50 GB or 1,000 files** to infer the schema for the input directory.  \n",
    "\n",
    "- It writes inferred schemas into a ```_schemas``` folder under the configured ```cloudFiles.schemaLocation```—this becomes your source of truth for schema evolution over time\n",
    "\n",
    "#### 2. Supported File Formats & Type Inference\n",
    "- **JSON, CSV, XML:** Auto Loader infers everything as strings unless you enable ```.option(\"cloudFiles.inferColumnTypes\", \"true\")```.  \n",
    "\n",
    "- **Text, Binary:** These formats don't support evolution.\n",
    "\n",
    "- **Parquet, Avro:** Typed formats retain their native data types and are merged during sampling. On type conflicts, it chooses the widest type (e.g., long over int), unless overridden by ```schemaHints```.\n",
    "\n",
    "#### 3. Automatic Schema Evolution  \n",
    "- When Auto Loader detects **new columns**, it:\n",
    "  1. Stops the stream with an `UnknownFieldException`.\n",
    "  2. Merges the new column(s) into the schema, placing them at the end.\n",
    "  3. Keeps existing column types intact.  \n",
    "- **Note:** Instructs a pipeline restart (e.g., via Lakeflow Jobs) to resume with updated schema.\n",
    "\n",
    "#### 4. ```cloudFiles.schemaEvolutionMode``` Control  \n",
    "Auto Loader supports several modes:\n",
    "\n",
    "| Mode | Behavior |\n",
    "|------|----------|\n",
    "| `addNewColumns` | (Default) Stream **fails**, schema file updated to include new columns. |\n",
    "| `rescue` | Stream continues; unexpected fields go into a `_rescued_data` column. |\n",
    "| `failOnNewColumns` | Stream fails; schema is NOT updated until manually changed. |\n",
    "| `none` | Stream continues; new columns are ignored unless `rescuedDataColumn` is set. |\n",
    "\n",
    "- The default behavior is `addNewColumns` unless a user-supplied schema is provided, in which case the default changes to `none`.\n",
    "\n",
    "#### 5. `_rescued_data` Column  \n",
    "- If using ```rescue``` mode (or enabling ```rescuedDataColumn```), unmatched fields are captured in a ```_rescued_data``` column rather than being dropped.  \n",
    "- You can rename this column via the ```rescuedDataColumn``` option.\n",
    "\n",
    "#### 6. Partition Columns Are Ignored in Drift  \n",
    "- Auto Loader can detect Hive-style partition columns (e.g., ```/date=2025-01-01/```).  \n",
    "- **Partition evolution is not supported**: new partitions will not be added to the schema unless manually specified using ```cloudFiles.partitionColumns```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0349f884-0eb0-4326-a9da-0601a664b0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How Research Informs the Project\n",
    "\n",
    "#### 1. Schema Enforcement & Evolution (Delta Lake)\n",
    "- Clarifies which schema changes Delta *handles automatically*—such as column additions, nullable upcasts, and nested fields—ensuring the agent avoids duplicated work.\n",
    "\n",
    "- Defines the boundaries of *unsupported changes* (e.g., column drops, renames, type conversions) that require explicit intervention, allowing the agent to detect and prompt for these specific adjustments.\n",
    "\n",
    "- Supports development of logic that differentiates between \"safe\" drift (handled by system) and \"risky\" drift (requiring agent-generated code), improving system reliability\n",
    "\n",
    "#### 2. Auto Loader Schema Management (Streaming Context)\n",
    "- Highlights how Auto Loader samples schemas and manages drift through ```_schemas``` metadata and ```cloudFiles.schemaEvolutionMode``` configurations (e.g., ```addNewColumns```, ```rescue```), which enables the agent to align its behavior with real-time ingestion pipelines.\n",
    "\n",
    "- Identifies scenarios—like data captured in ```_rescued_data```, unsupported partition-schema changes, or streaming failures—where the agent should suggest specific modes or extraction logic, enhancing streaming robustness."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Project_Planning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
